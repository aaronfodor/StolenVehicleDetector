{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset builder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "u_WyEZ940axq",
        "uI6XEy4F0ljR",
        "f_MlKGok0PBi",
        "YNXxnxdV03oM",
        "_qz44ln_0Nkc",
        "Mbhbpg7W1LBJ",
        "f5U_RJtv20UK",
        "SHc2hdPVlQCZ",
        "D9jHtrta1vdM",
        "B6Wmdqg__ofL",
        "RFG_ypfZ18E1",
        "32BSvP8pCmT5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtJeYzpKzN9v"
      },
      "source": [
        "# Dataset creator Notebook\n",
        "\n",
        ">This notebook downloads specific classes from the Open Images dataset with the desired cardinality. Then, it transform this set into .tfrecord format to make it possible to use with Tensorflow Object Detection API.\n",
        ">\n",
        ">The downloaded set can be saved into the mounted Drive folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOzEm7240GcG"
      },
      "source": [
        "## 1. Prepare the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_WyEZ940axq"
      },
      "source": [
        "### Clone MISC repos with Git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbW-Udpp4izl",
        "outputId": "f94608eb-3bc1-40ac-abfe-245cb5b17cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!git clone https://github.com/EscVM/OIDv4_ToolKit.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OIDv4_ToolKit'...\n",
            "remote: Enumerating objects: 422, done.\u001b[K\n",
            "remote: Total 422 (delta 0), reused 0 (delta 0), pack-reused 422\u001b[K\n",
            "Receiving objects: 100% (422/422), 34.08 MiB | 33.30 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI6XEy4F0ljR"
      },
      "source": [
        "### Install Object Detection module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX4MYxyxVfUF",
        "outputId": "9133ddd0-8bef-4866-c8ff-4566f286536c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!pip3 install tensorflow-object-detection-api"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-object-detection-api\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/11/7f6d3c5c4b603cc40b2813059779afb641bd5eb68045c62ca520bfce0359/tensorflow_object_detection_api-0.1.1.tar.gz (577kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (7.0.0)\n",
            "Requirement already satisfied: Matplotlib>=2.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (3.2.2)\n",
            "Requirement already satisfied: Cython>=0.28.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (0.29.21)\n",
            "Requirement already satisfied: Protobuf in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (3.12.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (4.2.6)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (1.0.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (2.3.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (0.5.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (0.35.1)\n",
            "Collecting twine\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/db/b2c65078b783c6694bdfa0911bbbe0e2be7fcbc98ff23a99b8be544906b6/twine-3.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from Protobuf->tensorflow-object-detection-api) (50.3.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from Protobuf->tensorflow-object-detection-api) (1.15.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (7.5.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (4.7.7)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.3.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.6.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.10.0)\n",
            "Collecting pkginfo>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/d5/451b913307b478c49eb29084916639dc53a88489b993530fed0a66bab8b9/pkginfo-1.5.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (2.0.0)\n",
            "Requirement already satisfied: tqdm>=4.14 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (4.41.1)\n",
            "Collecting keyring>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/ed/7be20815f248b0d6aae406783c2bee392640924623c4e17b50ca90c7f74d/keyring-21.4.0-py3-none-any.whl\n",
            "Collecting colorama>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (2.23.0)\n",
            "Collecting rfc3986>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting requests-toolbelt!=0.9.0,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hCollecting readme-renderer>=21.0\n",
            "  Downloading https://files.pythonhosted.org/packages/54/e4/ed43056d80a4fcc3667e543a59cc6beaf0a3c0eade837e5591e82ad3c25a/readme_renderer-26.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (4.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (5.0.7)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (19.0.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (5.3.5)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (1.9.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (3.2.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (2.11.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.4.4)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->tensorflow-object-detection-api) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (1.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->twine->tensorflow-object-detection-api) (3.2.0)\n",
            "Collecting SecretStorage>=3; sys_platform == \"linux\"\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/50/8a02cad020e949e6d7105f5f4530d41e3febcaa5b73f8f2148aacb3aeba5/SecretStorage-3.1.2-py3-none-any.whl\n",
            "Collecting jeepney>=0.4.2; sys_platform == \"linux\"\n",
            "  Downloading https://files.pythonhosted.org/packages/79/31/2e8d42727595faf224c6dbb748c32b192e212f25495fe841fb7ce8e168b8/jeepney-0.4.3-py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (1.24.3)\n",
            "Requirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.6/dist-packages (from readme-renderer>=21.0->twine->tensorflow-object-detection-api) (0.16)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets->jupyter->tensorflow-object-detection-api) (4.4.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->tensorflow-object-detection-api) (2.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->tensorflow-object-detection-api) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->tensorflow-object-detection-api) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->tensorflow-object-detection-api) (4.8.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->tensorflow-object-detection-api) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->tensorflow-object-detection-api) (20.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->tensorflow-object-detection-api) (1.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->tensorflow-object-detection-api) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->tensorflow-object-detection-api) (0.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (0.2.8)\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/62/30f6936941d87a5ed72efb24249437824f6b2c953901245b58c91fde2f27/cryptography-3.1.1-cp35-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->tensorflow-object-detection-api) (0.4.8)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->SecretStorage>=3; sys_platform == \"linux\"->keyring>=15.1->twine->tensorflow-object-detection-api) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->SecretStorage>=3; sys_platform == \"linux\"->keyring>=15.1->twine->tensorflow-object-detection-api) (2.20)\n",
            "Building wheels for collected packages: tensorflow-object-detection-api\n",
            "  Building wheel for tensorflow-object-detection-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-object-detection-api: filename=tensorflow_object_detection_api-0.1.1-cp36-none-any.whl size=844515 sha256=1b8fe322ad80e98470401558aebca291759151d43d9b58661eee20d25598660c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/54/d0/cfca11930c4b2025d40dede77059094070a67cc3e7bd3b285f\n",
            "Successfully built tensorflow-object-detection-api\n",
            "Installing collected packages: pkginfo, cryptography, jeepney, SecretStorage, keyring, colorama, rfc3986, requests-toolbelt, readme-renderer, twine, tensorflow-object-detection-api\n",
            "Successfully installed SecretStorage-3.1.2 colorama-0.4.3 cryptography-3.1.1 jeepney-0.4.3 keyring-21.4.0 pkginfo-1.5.0.1 readme-renderer-26.0 requests-toolbelt-0.9.1 rfc3986-1.4.0 tensorflow-object-detection-api-0.1.1 twine-3.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_MlKGok0PBi"
      },
      "source": [
        "### Import required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifQ3zcz03s2a"
      },
      "source": [
        "import imageio\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import contextlib2\n",
        "import gc\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from object_detection.utils import dataset_util"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXq0JpK31CjO"
      },
      "source": [
        "### Define paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBnx0Ibd2sG2"
      },
      "source": [
        "rootPath = '/content/drive/My Drive/Machine Learning/Stolen vehicle detection'\n",
        "dataPath = rootPath + '/data'\n",
        "\n",
        "localPath = '/content'\n",
        "rootOIDv4Path = localPath + '/OIDv4_ToolKit'\n",
        "datasetRootPath = rootOIDv4Path + '/OID'\n",
        "generatedDatasetPath = localPath + '/generated'\n",
        "recordsPath = generatedDatasetPath + '/records'\n",
        "csvPath = generatedDatasetPath + '/csv'"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNXxnxdV03oM"
      },
      "source": [
        "### Install OIDv4 requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXKClf7R4rky",
        "outputId": "4c2b807a-8c75-4796-d617-b2501f250d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "os.chdir(rootOIDv4Path)\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.18.5)\n",
            "Collecting awscli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/68/511b344c5d0a4ca99477c1c1aaf7a3c20fd65c16fc76e2602c90b5d758fe/awscli-1.18.153.tar.gz (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.41.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (4.1.2.30)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n",
            "Collecting botocore==1.18.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/d5/9d0db656de5bb8c431232d5976526ca5b6ac1e4563f95ce6cd842bbccff4/botocore-1.18.12-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 12.8MB/s \n",
            "\u001b[?25hCollecting docutils<0.16,>=0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 40.0MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.4,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli->-r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli->-r requirements.txt (line 3)) (0.4.3)\n",
            "Collecting rsa<=4.5.0,>=3.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/26/f8/8127fdda0294f044121d20aac7785feb810e159098447967a6103dedfb96/rsa-4.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.15.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=4.5.0,>=3.1.2->awscli->-r requirements.txt (line 3)) (0.4.8)\n",
            "Building wheels for collected packages: awscli\n",
            "  Building wheel for awscli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for awscli: filename=awscli-1.18.153-py2.py3-none-any.whl size=3354722 sha256=d6e9e9985461907c1ecb11bf8c97832054c8cd18814ca019ca3872b63746751e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/42/20/ce50d9438e1ce7655a3c2b928221afb57749bd140c74d70845\n",
            "Successfully built awscli\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, docutils, s3transfer, rsa, awscli\n",
            "  Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Found existing installation: rsa 4.6\n",
            "    Uninstalling rsa-4.6:\n",
            "      Successfully uninstalled rsa-4.6\n",
            "Successfully installed awscli-1.18.153 botocore-1.18.12 docutils-0.15.2 jmespath-0.10.0 rsa-4.5 s3transfer-0.3.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qz44ln_0Nkc"
      },
      "source": [
        "### Mount Google Drive to this Notebook instance\n",
        ">As the dataset has been prepared previously and updated to Google Drive, the model building and training process will be done there, not locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstP48rK3t7s",
        "outputId": "b257d4fc-a08c-4517-d726-13959f9ba6cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(localPath)\n",
        "# Show current directory\n",
        "!pwd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbhbpg7W1LBJ"
      },
      "source": [
        "##2. Get the data\n",
        ">There are 3 possibilities to get the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5U_RJtv20UK"
      },
      "source": [
        "#### A: extract a previously downloaded raw dataset from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CozVAFBQWZO",
        "outputId": "0ab9c459-2ce2-44a5-a481-2540ab1651c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Run it only when the dataset is not yet extracted\n",
        "'''\n",
        "zipRef = zipfile.ZipFile(dataPath + \"/dataset.zip\", 'r')\n",
        "zipRef.extractall(localPath + \"/data\")\n",
        "zipRef.close()\n",
        "print('Dataset extraction successful')\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset extraction successful\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHc2hdPVlQCZ"
      },
      "source": [
        "#### B: extract a previously downloaded raw dataset placed under `/content`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjThmNEcU10U",
        "outputId": "009106d5-a089-4455-ae2e-63eba00c34e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run it only when the dataset is not yet extracted \n",
        "'''\n",
        "zipRef = zipfile.ZipFile(\"/content/dataset.zip\", 'r')\n",
        "zipRef.extractall(localPath + \"/data\")\n",
        "zipRef.close()\n",
        "print('Dataset extraction successful')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset extraction successful\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9jHtrta1vdM"
      },
      "source": [
        "#### C: download from Open Image Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Wmdqg__ofL"
      },
      "source": [
        "##### show available OIDv4 commands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uodxYum5iPz",
        "outputId": "3b0b17c9-d0fa-488a-9390-fa0232b2cd56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "os.chdir(rootOIDv4Path)\n",
        "!python3 main.py -h"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: main.py [-h] [--Dataset /path/to/OID/csv/] [-y]\n",
            "               [--classes list of classes [list of classes ...]]\n",
            "               [--type_csv 'train' or 'validation' or 'test' or 'all']\n",
            "               [--sub Subset of human verified images or machine generated h or m)]\n",
            "               [--image_IsOccluded 1 or 0] [--image_IsTruncated 1 or 0]\n",
            "               [--image_IsGroupOf 1 or 0] [--image_IsDepiction 1 or 0]\n",
            "               [--image_IsInside 1 or 0] [--multiclasses 0 (default or 1]\n",
            "               [--n_threads [default 20]] [--noLabels]\n",
            "               [--limit integer number]\n",
            "               <command> 'downloader', 'visualizer' or 'ill_downloader'.\n",
            "\n",
            "Open Image Dataset Downloader\n",
            "\n",
            "positional arguments:\n",
            "  <command> 'downloader', 'visualizer' or 'ill_downloader'.\n",
            "                        'downloader', 'visualizer' or 'ill_downloader'.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --Dataset /path/to/OID/csv/\n",
            "                        Directory of the OID dataset folder\n",
            "  -y, --yes             ans Yes to possible download of missing files\n",
            "  --classes list of classes [list of classes ...]\n",
            "                        Sequence of 'strings' of the wanted classes\n",
            "  --type_csv 'train' or 'validation' or 'test' or 'all'\n",
            "                        From what csv search the images\n",
            "  --sub Subset of human verified images or machine generated (h or m)\n",
            "                        Download from the human verified dataset or from the\n",
            "                        machine generated one.\n",
            "  --image_IsOccluded 1 or 0\n",
            "                        Optional characteristic of the images. Indicates that\n",
            "                        the object is occluded by another object in the image.\n",
            "  --image_IsTruncated 1 or 0\n",
            "                        Optional characteristic of the images. Indicates that\n",
            "                        the object extends beyond the boundary of the image.\n",
            "  --image_IsGroupOf 1 or 0\n",
            "                        Optional characteristic of the images. Indicates that\n",
            "                        the box spans a group of objects (min 5).\n",
            "  --image_IsDepiction 1 or 0\n",
            "                        Optional characteristic of the images. Indicates that\n",
            "                        the object is a depiction.\n",
            "  --image_IsInside 1 or 0\n",
            "                        Optional characteristic of the images. Indicates a\n",
            "                        picture taken from the inside of the object.\n",
            "  --multiclasses 0 (default) or 1\n",
            "                        Download different classes separately (0) or together\n",
            "                        (1)\n",
            "  --n_threads [default 20]\n",
            "                        Num of the threads to use\n",
            "  --noLabels            No labels creations\n",
            "  --limit integer number\n",
            "                        Optional limit on number of images to download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFG_ypfZ18E1"
      },
      "source": [
        "##### Download train, test, validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYF-6oVf53jJ",
        "outputId": "8239e5ca-c054-495a-b2fa-3a185ca0f744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "!python3 main.py downloader --classes \"Vehicle registration plate\" --type_csv train --multiclasses 0 --limit 5368"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Vehicle registration plate.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the class-descriptions-boxable.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0mY\n",
            "...145%, 0 MB, 30461 KB/s, 0 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File class-descriptions-boxable.csv downloaded into OID/csv_folder/class-descriptions-boxable.csv.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the train-annotations-bbox.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0mY\n",
            "...100%, 1138 MB, 25956 KB/s, 44 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File train-annotations-bbox.csv downloaded into OID/csv_folder/train-annotations-bbox.csv.\u001b[0m\n",
            "\n",
            "\u001b[95mVehicle registration plate\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 5368 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 5368 images in train.\u001b[0m\n",
            "100% 5368/5368 [49:35<00:00,  1.80it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Vehicle registration plate of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rAMPicJImvJ",
        "outputId": "d153b65f-9d47-4c55-ea6a-a0193c6e379c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "!python3 main.py downloader --classes \"Vehicle registration plate\" --type_csv test --multiclasses 0 --limit 1113"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Vehicle registration plate.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the test-annotations-bbox.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0mY\n",
            "...100%, 49 MB, 25649 KB/s, 1 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File test-annotations-bbox.csv downloaded into OID/csv_folder/test-annotations-bbox.csv.\u001b[0m\n",
            "\n",
            "\u001b[95mVehicle registration plate\u001b[0m\n",
            "    [INFO] | Downloading test images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 1113 online images for test.\u001b[0m\n",
            "    [INFO] | Download of 1113 images in test.\u001b[0m\n",
            "100% 1113/1113 [10:18<00:00,  1.80it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Vehicle registration plate of test.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcloJ-6BIoYK",
        "outputId": "7d9a5c4e-612f-4645-d2b5-876b78f45c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "!python3 main.py downloader --classes \"Vehicle registration plate\" --type_csv validation --multiclasses 0 --limit 386"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Vehicle registration plate.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the validation-annotations-bbox.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0mY\n",
            "...100%, 16 MB, 11274 KB/s, 1 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File validation-annotations-bbox.csv downloaded into OID/csv_folder/validation-annotations-bbox.csv.\u001b[0m\n",
            "\n",
            "\u001b[95mVehicle registration plate\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 386 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 386 images in validation.\u001b[0m\n",
            "100% 386/386 [03:49<00:00,  1.68it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Vehicle registration plate of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32BSvP8pCmT5"
      },
      "source": [
        "##### Save the downloaded set to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJaFDxbfCqyP",
        "outputId": "e07a99b8-19ed-4193-cee3-6ffdf2c0f868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "shutil.make_archive(\"/content/dataset\", 'zip', datasetRootPath)\n",
        "shutil.move(\"/content/dataset.zip\", dataPath + \"/dataset.zip\")\n",
        "print(f'Dataset successfully zipped & moved to: {dataPath}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset successfully zipped & moved to: /content/drive/My Drive/Machine Learning/Stolen vehicle detection/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07IlsSmC2Clt"
      },
      "source": [
        "##3. Create supplementary files\n",
        ">Create a `txt` file containing the class names, and `tfrecord`s containing the images and their annotations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R2ykpd91kdH"
      },
      "source": [
        "### Define class names to process\n",
        ">Order of the classes counts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CfOyguNZE_q"
      },
      "source": [
        "classNames = [\"Vehicle registration plate\", \"Vehicle\"]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDopIc4p2n4O"
      },
      "source": [
        "### Create class names file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMZ3ezQlYH03"
      },
      "source": [
        "#### Create class names `txt` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqCdWeACXr1A"
      },
      "source": [
        "def createClassTxt(destinationDir, classNames):\n",
        "\n",
        "  os.chdir(destinationDir)\n",
        "  # creating the `classes.txt` file\n",
        "  classesPath = os.path.join(\"classes.txt\")\n",
        "\n",
        "  content = \"\"\n",
        "\n",
        "  # creates a txt file that contains the class names\n",
        "  for i, className in enumerate(classNames):\n",
        "\n",
        "      content = (\n",
        "          content + \n",
        "          f\"{className}\\n\"\n",
        "      )\n",
        "\n",
        "  content = content.strip()\n",
        "\n",
        "  with open(classesPath, \"w\") as f:\n",
        "      f.write(content)\n",
        "\n",
        "  print(\"Class names txt file creation successful. Destination: %s\" %(destinationDir + '/' + classesPath))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wXB0UsTYPBh"
      },
      "source": [
        "#### Create class names `pbtxt` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMU8M5FCXvu0"
      },
      "source": [
        "def createClassPbtxt(destinationDir, classNames):\n",
        "\n",
        "  os.chdir(destinationDir)\n",
        "  # creating the `classes.pbtxt` file\n",
        "  classesPath = os.path.join(\"classes.pbtxt\")\n",
        "\n",
        "  content = \"\"\n",
        "\n",
        "  # creates a pbtxt file that contains the class names\n",
        "  for i, className in enumerate(classNames):\n",
        "\n",
        "      content = (\n",
        "          content + \n",
        "          f\"item {{\\n    id: {i + 1}\\n    name: '{className}'\\n }}\\n\\n\"\n",
        "      )\n",
        "\n",
        "  content = content.strip()\n",
        "\n",
        "  with open(classesPath, \"w\") as f:\n",
        "      f.write(content)\n",
        "\n",
        "  print(\"Class names pbtxt file creation successful. Destination: %s\" %(destinationDir + '/' + classesPath))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4_FUcGU2_Gl"
      },
      "source": [
        "### Create the `tfrecord` files\n",
        ">These files are the inputs of the Tensorflow Object Detection API models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMFpGl_G3aeI"
      },
      "source": [
        "#### Create one `tfrecord` from the input parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFa2Tzh0mt1J"
      },
      "source": [
        "'''\n",
        "Creates the tfrecord element from the input.\n",
        "\n",
        "  Args:\n",
        "    path: path of the image\n",
        "    imageId: Id of the current image to create records for\n",
        "    mustPresentClasses: list of class names that must present (returns empty record otherwise)\n",
        "    classes: dict of classes that is added to the record if present\n",
        "    imageAnnotations: dataframe containing the image BBs\n",
        "\n",
        "  Returns:\n",
        "    TFRecord\n",
        "'''\n",
        "def createRecord(path, imageId, mustPresentClasses, classes, imageAnnotations):\n",
        "\n",
        "  image = Image.open(path)\n",
        "  imgWidth, imgHeight = image.size\n",
        "  imgData = tf.io.gfile.GFile(path, 'rb').read()\n",
        "\n",
        "  xmins = []\n",
        "  xmaxs = []\n",
        "  ymins = []\n",
        "  ymaxs = []\n",
        "  classesText = []\n",
        "  classesInt = []\n",
        "\n",
        "  for mustClass in mustPresentClasses:\n",
        "\n",
        "    if(mustClass not in imageAnnotations['LabelName'].values):\n",
        "      return None\n",
        "\n",
        "  for _, row in imageAnnotations.loc[imageAnnotations['LabelName'].isin(classes.keys())].iterrows():\n",
        "\n",
        "      xmins.append(row['XMin'])\n",
        "      xmaxs.append(row['XMax'])\n",
        "      ymins.append(row['YMin'])\n",
        "      ymaxs.append(row['YMax'])\n",
        "      classesText.append(row['LabelName'].encode('utf8'))\n",
        "      classesInt.append(classes[row['LabelName']])\n",
        "\n",
        "  tFRecord = tf.train.Example(features=tf.train.Features(feature={\n",
        "      'image/height': dataset_util.int64_feature(imgHeight),\n",
        "      'image/width': dataset_util.int64_feature(imgWidth),\n",
        "      'image/filename': dataset_util.bytes_feature(imageId.encode('utf8')),\n",
        "      'image/source_id': dataset_util.bytes_feature(imageId.encode('utf8')),\n",
        "      'image/encoded': dataset_util.bytes_feature(imgData),\n",
        "      'image/format': dataset_util.bytes_feature(b'jpg'),\n",
        "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "      'image/object/class/text': dataset_util.bytes_list_feature(classesText),\n",
        "      'image/object/class/label': dataset_util.int64_list_feature(classesInt)\n",
        "  }))\n",
        "\n",
        "  return tFRecord"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTmGz4IB-zWJ"
      },
      "source": [
        "#### Create a `csv` row from the input parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzBSRxCWkbA"
      },
      "source": [
        "##### Encode list elements as a row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVdARmgcOaiQ"
      },
      "source": [
        "def appendListAsRow(fileName, list):\n",
        "\n",
        "    # Open file in append mode\n",
        "    with open(fileName, 'a+', newline='') as writeObj:\n",
        "\n",
        "      # Create a writer object from csv module\n",
        "      writer = csv.writer(writeObj)\n",
        "\n",
        "      # Add contents of list as last row in the csv file\n",
        "      writer.writerow(list)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm_BPmSFWs1m"
      },
      "source": [
        "##### Create `csv` rows of an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uVAP7K2-ume"
      },
      "source": [
        "def createCsvRowsOfImage(imgPath, imageId, classes, annotations):\n",
        "\n",
        "  image = Image.open(imgPath)\n",
        "  imgWidth, imgHeight = image.size\n",
        "  imgData = 1\n",
        "  extension = \"jpg\"\n",
        "\n",
        "  imageAnnotations = annotations.loc[annotations['ImageID'] == imageId]\n",
        "\n",
        "  CsvRows = []\n",
        "\n",
        "  for _, row in imageAnnotations.loc[imageAnnotations['LabelName'].isin(classes.keys())].iterrows():\n",
        "\n",
        "    CsvRow = []\n",
        "\n",
        "    CsvRow.append(imgHeight)\n",
        "    CsvRow.append(imgWidth)\n",
        "    CsvRow.append(imageId)\n",
        "    CsvRow.append(imageId)\n",
        "    CsvRow.append(imgData)\n",
        "    CsvRow.append(extension)\n",
        "    CsvRow.append(row['XMin'])\n",
        "    CsvRow.append(row['XMax'])\n",
        "    CsvRow.append(row['YMin'])\n",
        "    CsvRow.append(row['YMax'])\n",
        "    CsvRow.append(row['LabelName'].encode('utf8'))\n",
        "    CsvRow.append(classes[row['LabelName']])\n",
        "\n",
        "    CsvRows.append(CsvRow)\n",
        "\n",
        "  return CsvRows"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NEhN5zWXPsu"
      },
      "source": [
        "#### Add `csv` file header"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_nQw4NKXOCX"
      },
      "source": [
        "def csvAddHeader(fileName):\n",
        "\n",
        "  headerRow = []\n",
        "\n",
        "  headerRow.append('imgHeight')\n",
        "  headerRow.append('imgWidth')\n",
        "  headerRow.append('imageId')\n",
        "  headerRow.append('imageId')\n",
        "  headerRow.append('imgData')\n",
        "  headerRow.append('extension')\n",
        "  headerRow.append('XMin')\n",
        "  headerRow.append('XMax')\n",
        "  headerRow.append('YMin')\n",
        "  headerRow.append('YMax')\n",
        "  headerRow.append('LabelName')\n",
        "  headerRow.append('LabelId')\n",
        "\n",
        "  appendListAsRow(fileName, headerRow)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaP_6pKd3szr"
      },
      "source": [
        "#### Generate the `tfrecord` files from the provided images & annotations\n",
        ">The `numShards` variable serves for deciding how many sub-files must be created in order to speed up processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aih54jYsizdR"
      },
      "source": [
        "def open_sharded_tfrecord_output(exit_stack, base_path, num_shards):\n",
        "  \"\"\"Opens all TFRecord shards for writing and adds them to an exit stack.\n",
        "\n",
        "  Args:\n",
        "    exit_stack: A context2.ExitStack used to automatically closed the TFRecords\n",
        "      opened in this function.\n",
        "    base_path: The base path for all shards\n",
        "    num_shards: The number of shards\n",
        "\n",
        "  Returns:\n",
        "    The list of opened TFRecords. Position k in the list corresponds to shard k.\n",
        "  \"\"\"\n",
        "  tf_record_output_filenames = [\n",
        "      '{}-{:05d}-of-{:05d}'.format(base_path, idx, num_shards)\n",
        "      for idx in range(num_shards)\n",
        "  ]\n",
        "\n",
        "  tfrecords = [\n",
        "      exit_stack.enter_context(tf.io.TFRecordWriter(file_name))\n",
        "      for file_name in tf_record_output_filenames\n",
        "  ]\n",
        "\n",
        "  return tfrecords"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTvkoo8Cn8v6"
      },
      "source": [
        "def open_sharded_csv_output(csvDir, numShards):\n",
        "  csvFileNames = []\n",
        "\n",
        "  for i in range(0, numShards):\n",
        "    csvFileNames.append('{}-{:05d}-of-{:05d}.csv'.format(csvDir, i, numShards))\n",
        "\n",
        "  return csvFileNames"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y31iep6ww_Or"
      },
      "source": [
        "def tfrecordGenerator(\n",
        "    classesFile,\n",
        "    classDescriptionsFile,\n",
        "    annotationsFile,\n",
        "    imagesDir,\n",
        "    outputFile,\n",
        "    numShards,\n",
        "    csvNeeded,\n",
        "    csvDir\n",
        "):\n",
        "\n",
        "    classes = list(filter(None, open(classesFile).read().split('\\n')))\n",
        "    classes = {name: idx + 1 for idx, name in enumerate(classes)}\n",
        "    nameMustPresentClasses = [\"Vehicle registration plate\"]\n",
        "\n",
        "    classDescriptions = {row[0]: row[1] for _, row in pd.read_csv(classDescriptionsFile, header=None).iterrows()}\n",
        "\n",
        "    annotations = pd.read_csv(annotationsFile)\n",
        "    annotations['ImageID'] = annotations['ImageID'].astype(str)\n",
        "    annotations['LabelName'] = annotations['LabelName'].map(lambda n: classDescriptions[n])\n",
        "\n",
        "    #list of labels to rename to Vehicle\n",
        "    toReplace = [\"Car\", \"Airplane\", \"Helicopter\", \"Boat\", \"Motorcycle\", \"Bus\", \"Taxi\", \"Truck\", \"Ambulance\"]\n",
        "\n",
        "    #generate list of labels to keep\n",
        "    toKeep = toReplace.copy()\n",
        "    for item in nameMustPresentClasses:\n",
        "      toKeep.append(item)\n",
        "    #keep only labels in toKeep\n",
        "    annotations = annotations[annotations['LabelName'].isin(toKeep)]\n",
        "\n",
        "    #change toReplace labels to Vehicle\n",
        "    annotations.loc[annotations['LabelName'].isin(toReplace), 'LabelName'] = \"Vehicle\"\n",
        "\n",
        "    images = tf.io.gfile.glob(imagesDir + '/*/*.jpg')\n",
        "    images = map(lambda i: (os.path.basename(i).split('.jpg')[0], i), images)\n",
        "    images = dict(images)\n",
        "    print(f'{len(images)} images found')\n",
        "\n",
        "    with contextlib2.ExitStack() as tfRecordCloseStack:\n",
        "\n",
        "      outputRecords = open_sharded_tfrecord_output(tfRecordCloseStack, outputFile, numShards)\n",
        "      outputCsvs = open_sharded_csv_output(csvDir, numShards)\n",
        "\n",
        "      index = 0\n",
        "\n",
        "      if(csvNeeded == True):\n",
        "        for filePath in outputCsvs: \n",
        "          csvAddHeader(filePath)\n",
        "\n",
        "      for imageId, path in images.items():\n",
        "\n",
        "        imageAnnotations = annotations.loc[annotations['ImageID'] == imageId]\n",
        "        TFRecord = createRecord(path, imageId, nameMustPresentClasses, classes, imageAnnotations)\n",
        "\n",
        "        if(TFRecord is None):\n",
        "          continue\n",
        "\n",
        "        outputShardIndex = index % numShards\n",
        "        outputRecords[outputShardIndex].write(TFRecord.SerializeToString())\n",
        "\n",
        "        if(csvNeeded == True):\n",
        "          imageRows = createCsvRowsOfImage(path, imageId, classes, annotations)\n",
        "          for row in imageRows:\n",
        "            appendListAsRow(outputCsvs[outputShardIndex], row)\n",
        "\n",
        "        index += 1\n",
        "\n",
        "    print('TFRecords has been successfully created')"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ivkngm8kT0"
      },
      "source": [
        "##4. Generate dataset\n",
        ">The generated `tfrecord` files need to be zipped and saved to the Drive directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD7J4G6Q_plN"
      },
      "source": [
        "#### Create output directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4_Gx1td_nMr"
      },
      "source": [
        "if not os.path.exists(recordsPath):\n",
        "  os.makedirs(recordsPath)\n",
        "\n",
        "if not os.path.exists(csvPath):\n",
        "  os.makedirs(csvPath)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQYorSQ15X5n"
      },
      "source": [
        "#### Generate train, test, validation `tfrecord` files\n",
        ">`subsetTypes` contains the subset names, `numShards` contains their corresponding values to be sharded into. More than a few thousand `tfrecord` rows dramatically slow down the process. `numShards=1` means sharding is not needed at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jv5i6EERGpT",
        "outputId": "2b7af8d0-22f8-4ebf-b3d1-95070c5987e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# if the data is newly downloaded, use this:\n",
        "#dataSourcePath = datasetRootPath\n",
        "# if the data has been extracted, use this:\n",
        "dataSourcePath = localPath + \"/data\"\n",
        "\n",
        "createClassTxt(dataSourcePath, classNames)\n",
        "createClassPbtxt(recordsPath, classNames)\n",
        "\n",
        "# train, test, validation subsets\n",
        "subsetTypes = ['train', 'test', 'validation']\n",
        "numShards = [10, 3, 1]\n",
        "\n",
        "csvNeeded = True\n",
        "csvDirName = \"\"\n",
        "\n",
        "for subsetType, numShard in zip(subsetTypes, numShards):\n",
        "\n",
        "  print(f'***Generating {subsetType} tfrecord files [sharded into {numShard} piece(s)]***')\n",
        "\n",
        "  classesFile = dataSourcePath + '/classes.txt'\n",
        "  classDescriptionsFile = dataSourcePath + '/csv_folder/class-descriptions-boxable.csv'\n",
        "  annotationsFile = dataSourcePath + '/csv_folder/' + subsetType + '-annotations-bbox.csv'\n",
        "  imagesDir = dataSourcePath + '/Dataset/' + subsetType\n",
        "  outputFile = recordsPath + '/' + subsetType + 'Dataset.tfrecord'\n",
        "  csvDirName = csvPath + '/' + subsetType\n",
        "\n",
        "  tfrecordGenerator(\n",
        "      classesFile,\n",
        "      classDescriptionsFile,\n",
        "      annotationsFile,\n",
        "      imagesDir,\n",
        "      outputFile,\n",
        "      numShard,\n",
        "      csvNeeded,\n",
        "      csvDirName\n",
        "  )\n",
        "\n",
        "  print('')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class names txt file creation successful. Destination: /content/data/classes.txt\n",
            "Class names pbtxt file creation successful. Destination: /content/generated/records/classes.pbtxt\n",
            "***Generating train tfrecord files [sharded into 10 piece(s)]***\n",
            "5368 images found\n",
            "TFRecords has been successfully created\n",
            "\n",
            "***Generating test tfrecord files [sharded into 3 piece(s)]***\n",
            "1113 images found\n",
            "TFRecords has been successfully created\n",
            "\n",
            "***Generating validation tfrecord files [sharded into 1 piece(s)]***\n",
            "386 images found\n",
            "TFRecords has been successfully created\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtAaxLiI9IYc"
      },
      "source": [
        "###Zip & save `tfrecord` files to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrYaci2wsJxF",
        "outputId": "3eb259a3-a7cf-41ee-9512-010a5d785eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "outputFileName = \"datasetRecords\"\n",
        "os.chdir(localPath)\n",
        "shutil.make_archive(outputFileName, 'zip', recordsPath)\n",
        "shutil.move(localPath + '/' + outputFileName + '.zip', dataPath)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Machine Learning/Stolen vehicle detection/data/datasetRecords.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOYsm0uGVIhI"
      },
      "source": [
        "###Zip & save `csv` files to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UYd6ThDVHkd",
        "outputId": "47c5cf79-498d-4c4b-ac1a-4416435a01d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "outputFileName = \"datasetCsvs\"\n",
        "os.chdir(localPath)\n",
        "shutil.make_archive(outputFileName, 'zip', csvPath)\n",
        "shutil.move(localPath + '/' + outputFileName + '.zip', dataPath)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Machine Learning/Stolen vehicle detection/data/datasetCsvs.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkfc8ts89ZB8"
      },
      "source": [
        "### How to use the sharded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNOd_QH0f4bB"
      },
      "source": [
        "tf_record_input_reader {\n",
        "  input_path: \"/path/to/trainDataset.tfrecord-?????-of-00010\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqu3q4YLWOfc"
      },
      "source": [
        "##Force garbage collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oB7Dol-TuNU",
        "outputId": "3f9136b4-3a75-47fb-a2d5-970378b66e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "759"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    }
  ]
}