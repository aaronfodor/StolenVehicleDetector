{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb-yVSw2wGKs"
      },
      "source": [
        "#1 Environment preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IQj5AfRnlAmC"
      },
      "outputs": [],
      "source": [
        "#external dependencies\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import math\r\n",
        "import os\r\n",
        "import urllib\r\n",
        "import zipfile\r\n",
        "from pathlib import Path\r\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "56IcSzLOwGK3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Windows (local) settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Edit this to point to project root\r\n",
        "sep = \"\\\\\"\r\n",
        "home = f\"B:{sep}Dev{sep}GitHub{sep}University{sep}ThesisMSc{sep}StolenVehicleDetector{sep}Machine Learning{sep}OCR\"\r\n",
        "data_dir = home + f\"{sep}data\"\r\n",
        "model_dir = home + f\"{sep}model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linux (colab) settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "sep = \"/\"\r\n",
        "#Edit this to point to project root\r\n",
        "home = f\"{sep}content{sep}OCR\"\r\n",
        "data_dir = home + f\"{sep}data\"\r\n",
        "model_dir = home + f\"{sep}model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tEZMhTNt5gcR"
      },
      "outputs": [],
      "source": [
        "#Setup project directories\r\n",
        "if not os.path.exists(home):\r\n",
        "    os.makedirs(home)\r\n",
        "\r\n",
        "if not os.path.exists(data_dir):\r\n",
        "    os.makedirs(data_dir)\r\n",
        "\r\n",
        "if not os.path.exists(model_dir):\r\n",
        "    os.makedirs(model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV-Ea1KUwGK7"
      },
      "source": [
        "#2 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeRiIx5P7FEz"
      },
      "source": [
        "##Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBlZN4lryGbH"
      },
      "source": [
        "###Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "flvXWcafwGKz"
      },
      "outputs": [],
      "source": [
        "class AbstractDatasetWrapper:\r\n",
        "    '''Abstract dataset wrapper class\r\n",
        "    '''\r\n",
        "    dataset_name = \"\"\r\n",
        "    source_url: str = \"\"\r\n",
        "\r\n",
        "    def __init__(self, data_dir: str) -> None:\r\n",
        "        self.data_dir = data_dir\r\n",
        "        self.dataset_dir = self.data_dir + \"/\" + self.dataset_name\r\n",
        "        self.images = []\r\n",
        "        self.labels = []\r\n",
        "        self.unique_characters: set = set()\r\n",
        "        self.label_max_length: int = 0\r\n",
        "\r\n",
        "    def __download_dataset(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def __extract_dataset(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def prepare(self):\r\n",
        "        '''Prepare the whole dataset before working with it\r\n",
        "        '''\r\n",
        "        pass\r\n",
        "\r\n",
        "    def show_info(self):\r\n",
        "        '''Show dataset properties\r\n",
        "        '''\r\n",
        "        print(\"Number of images: \", len(self.images))\r\n",
        "        print(\"Number of labels: \", len(self.labels))\r\n",
        "        print(\"Longest label: \", self.label_max_length)\r\n",
        "        print(\"Number of unique characters: \", len(self.unique_characters))\r\n",
        "        print(\"Characters present: \", self.unique_characters)\r\n",
        "\r\n",
        "\r\n",
        "class CaptchaDatasetWrapper(AbstractDatasetWrapper):\r\n",
        "    '''The captcha dataset wrapper class\r\n",
        "    '''\r\n",
        "\r\n",
        "    dataset_name = \"captcha_images_v2\"\r\n",
        "    source_url: str = \"https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip\"\r\n",
        "\r\n",
        "    def __init__(self, data_dir: str) -> None:\r\n",
        "        super().__init__(data_dir)\r\n",
        "        self.download_file_name = self.dataset_dir + \".zip\"\r\n",
        "\r\n",
        "    def __download_dataset(self):\r\n",
        "        download_file_path = Path(self.download_file_name)\r\n",
        "        if not download_file_path.exists():\r\n",
        "            #Not yet downloaded\r\n",
        "            urllib.request.urlretrieve(self.source_url, download_file_path)\r\n",
        "\r\n",
        "    def __extract_dataset(self):\r\n",
        "        download_file_path = Path(self.download_file_name)\r\n",
        "        if download_file_path.exists():\r\n",
        "            #if the target data directory is empty\r\n",
        "            if not os.listdir(self.data_dir):\r\n",
        "                with zipfile.ZipFile(self.download_file_name, 'r') as zip_ref:\r\n",
        "                    zip_ref.extractall(path = self.data_dir)\r\n",
        "\r\n",
        "    def prepare(self):\r\n",
        "        '''Prepare the whole dataset before working with itP\r\n",
        "        '''\r\n",
        "        #Download & extract dataset if it has not been yet\r\n",
        "        self.__download_dataset()\r\n",
        "        #self.__extract_dataset()\r\n",
        "\r\n",
        "        dataset_path = Path(self.dataset_dir)\r\n",
        "        #Get list of the images\r\n",
        "        self.images = sorted(list(map(str, list(dataset_path.glob(\"*.png\")))))\r\n",
        "        #Labels of images: image names minus \".png\"\r\n",
        "        self.labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in self.images]\r\n",
        "        #Set of distinct characters in the labels\r\n",
        "        self.unique_characters = set(char for label in self.labels for char in label)\r\n",
        "        #Compute the longest label in the dataset\r\n",
        "        self.label_max_length = max([len(label) for label in self.labels])\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIDlqNONyMIL"
      },
      "source": [
        "###Controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "049wQTl2wGK0"
      },
      "outputs": [],
      "source": [
        "class DatasetController:\r\n",
        "    '''Dataset controller class (to split & transform data)\r\n",
        "    '''\r\n",
        "    def __init__(self, data_wrapper: AbstractDatasetWrapper, img_n: int, channels: int):\r\n",
        "        self.data_wrapper = data_wrapper\r\n",
        "        # Mapping characters to integers\r\n",
        "        self.char_to_num = layers.experimental.preprocessing.StringLookup(\r\n",
        "            vocabulary=list(self.data_wrapper.unique_characters), num_oov_indices=0, mask_token=None)\r\n",
        "        # Mapping integers back to original characters\r\n",
        "        self.num_to_char = layers.experimental.preprocessing.StringLookup(\r\n",
        "            vocabulary=self.char_to_num.get_vocabulary(), mask_token=None, invert=True)\r\n",
        "        #Required image dimensions\r\n",
        "        self.img_n = img_n\r\n",
        "        self.channels = channels\r\n",
        "        \r\n",
        "        #Subsets\r\n",
        "        self.train_dataset = []\r\n",
        "        self.validation_dataset = []\r\n",
        "        #Additional generated dataset info\r\n",
        "        self.batch_size = 0\r\n",
        "        self.train_ratio = 0\r\n",
        "\r\n",
        "    def split_data(self, batch_size, train_ratio=0.9, shuffle=True):\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.train_ratio = train_ratio\r\n",
        "        #Get the total size of the input dataset\r\n",
        "        size = len(self.data_wrapper.images)\r\n",
        "        #Make an indices array and shuffle it, if required\r\n",
        "        indices = np.arange(size)\r\n",
        "        if shuffle:\r\n",
        "            np.random.shuffle(indices)\r\n",
        "        #Get the size of training samples\r\n",
        "        train_samples = int(size * train_ratio)\r\n",
        "\r\n",
        "        #Split data into training and validation sets\r\n",
        "        images = np.array(self.data_wrapper.images)\r\n",
        "        labels = np.array(self.data_wrapper.labels)\r\n",
        "\r\n",
        "        x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\r\n",
        "        x_val, y_val = images[indices[train_samples:]], labels[indices[train_samples:]]\r\n",
        "\r\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n",
        "        train_dataset = (\r\n",
        "            train_dataset.map(\r\n",
        "                self.encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "            .batch(self.batch_size)\r\n",
        "            .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n",
        "        )\r\n",
        "\r\n",
        "        validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\n",
        "        validation_dataset = (\r\n",
        "            validation_dataset.map(\r\n",
        "                self.encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "            .batch(self.batch_size)\r\n",
        "            .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n",
        "        )\r\n",
        "\r\n",
        "        self.train_dataset = train_dataset\r\n",
        "        self.validation_dataset = validation_dataset\r\n",
        "\r\n",
        "        return train_dataset, validation_dataset\r\n",
        "\r\n",
        "    def encode_single_sample(self, img_path, label):\r\n",
        "        print(img_path)\r\n",
        "        #Read the image\r\n",
        "        image = tf.io.read_file(img_path)\r\n",
        "        print(image)\r\n",
        "        #Decode and convert to the appropriate channels\r\n",
        "        image = tf.io.decode_png(image, channels=self.channels)\r\n",
        "        #Convert to float32 and normalize to the [0, 1) range\r\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\r\n",
        "        #Resize to the desired size\r\n",
        "        image = tf.image.resize_with_pad(image, self.img_n, self.img_n)\r\n",
        "        #Map the label characters to numbers\r\n",
        "        label = self.char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\r\n",
        "        #Return a dict as the model is expecting two inputs\r\n",
        "        return {\"image\": image, \"label\": label}\r\n",
        "\r\n",
        "    def encode_single_sample2(self, img_path, label):\r\n",
        "        #Read the image\r\n",
        "        image = Image.open(img_path)\r\n",
        "\r\n",
        "        if(self.channels == 1):\r\n",
        "            #Convert to grayscale\r\n",
        "            image = image.convert('gray')\r\n",
        "\r\n",
        "        #Convert to float32 and normalize to the [0, 1) range\r\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\r\n",
        "        #Resize to the desired size\r\n",
        "        image = tf.image.resize_with_pad(image, self.img_n, self.img_n)\r\n",
        "        #Map the label characters to numbers\r\n",
        "        label = self.char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\r\n",
        "        #Return a dict as the model is expecting two inputs\r\n",
        "        return {\"image\": image, \"label\": label}\r\n",
        "\r\n",
        "    def show_train_samples(self):\r\n",
        "        self.__show_batch_sample(self.train_dataset)\r\n",
        "\r\n",
        "    def show_validation_samples(self):\r\n",
        "        self.__show_batch_sample(self.validation_dataset)\r\n",
        "\r\n",
        "    def __show_batch_sample(self, dataset):\r\n",
        "        _, ax = plt.subplots(math.ceil(self.batch_size/4), 4, figsize=(12, 3*(self.batch_size/4)))\r\n",
        "\r\n",
        "        for batch in dataset.take(1):\r\n",
        "            images = batch[\"image\"]\r\n",
        "            labels = batch[\"label\"]\r\n",
        "            for i in range(self.batch_size):\r\n",
        "                sample = self.encode_single_sample(images[i], labels[i])\r\n",
        "                img = (sample[\"image\"] * 255).numpy().astype(\"uint8\")\r\n",
        "                label = tf.strings.reduce_join(self.num_to_char(sample[\"label\"])).numpy().decode(\"utf-8\")\r\n",
        "                ax[i // 4, i % 4].imshow(img)\r\n",
        "                ax[i // 4, i % 4].set_title(label)\r\n",
        "                ax[i // 4, i % 4].axis(\"off\")\r\n",
        "            plt.show()\r\n",
        "\r\n",
        "    def show_inference_results(self, model):\r\n",
        "        #Check results on validation samples\r\n",
        "        for batch in self.validation_dataset.take(1):\r\n",
        "            batch_images = batch[\"image\"]\r\n",
        "            batch_labels = batch[\"label\"]\r\n",
        "\r\n",
        "            predictions = model.predict(batch_images)\r\n",
        "            predicted_texts = self.decode_batch_predictions(predictions)\r\n",
        "\r\n",
        "            original_texts = []\r\n",
        "            for label in batch_labels:\r\n",
        "                label = tf.strings.reduce_join(self.num_to_char(label)).numpy().decode(\"utf-8\")\r\n",
        "                original_texts.append(label)\r\n",
        "\r\n",
        "            _, ax = plt.subplots(math.ceil(self.batch_size/4), 4, figsize=(12, 3*(self.batch_size/4)))\r\n",
        "            for i in range(len(predicted_texts)):\r\n",
        "                img = (batch_images[i] * 255).numpy().astype(np.uint8)\r\n",
        "                title = f\"Pred: {predicted_texts[i]}\"\r\n",
        "                ax[i // 4, i % 4].imshow(img)\r\n",
        "                ax[i // 4, i % 4].set_title(title)\r\n",
        "                ax[i // 4, i % 4].axis(\"off\")\r\n",
        "\r\n",
        "            plt.show()\r\n",
        "\r\n",
        "    def decode_batch_predictions(self, predictions):\r\n",
        "        '''A utility function to decode the output of the network\r\n",
        "        '''\r\n",
        "        input_length = np.ones(predictions.shape[0]) * predictions.shape[1]\r\n",
        "        #Greedy search is used\r\n",
        "        #For complex tasks where language models count, beam search can be used\r\n",
        "        results = keras.backend.ctc_decode(predictions, input_length=input_length, greedy=True)[0][0][:, :self.label_length]\r\n",
        "        raw_output = []\r\n",
        "        #Iterate over the results and get back the text\r\n",
        "        output_text = []\r\n",
        "        for res in results:\r\n",
        "            raw_output.append(res)\r\n",
        "            res = self.num_to_char(res).numpy()\r\n",
        "            output_text.append(res)\r\n",
        "            \r\n",
        "        return output_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PVgHMCq7AKG"
      },
      "source": [
        "##Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUZ3hqkFwGK7",
        "outputId": "c6d627a3-12ed-424a-8a7d-250a652948e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images:  16\n",
            "Number of labels:  16\n",
            "Longest label:  2\n",
            "Number of unique characters:  11\n",
            "Characters present:  {'a', '1', '3', '9', '6', '4', '8', '0', '5', '2', '7'}\n"
          ]
        }
      ],
      "source": [
        "data_wrapper = CaptchaDatasetWrapper(data_dir)\r\n",
        "data_wrapper.prepare()\r\n",
        "data_wrapper.show_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4yERz_Ki7nLe"
      },
      "outputs": [],
      "source": [
        "#Required input image dimensions (N x N images)\r\n",
        "img_n = 200\r\n",
        "channels = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WIA7HZm70CX1"
      },
      "outputs": [],
      "source": [
        "#Batch size for training and validation\r\n",
        "batch_size = 8\r\n",
        "#number of characters in the dataset\r\n",
        "num_characters = len(data_wrapper.unique_characters)\r\n",
        "#Training set ratio of all the images\r\n",
        "train_ratio = 0.5\r\n",
        "shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NVRtrsGn0CjG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
            "Tensor(\"ReadFile:0\", shape=(), dtype=string)\n",
            "Tensor(\"args_0:0\", shape=(), dtype=string)\n",
            "Tensor(\"ReadFile:0\", shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "dataset_controller = DatasetController(data_wrapper, img_n, channels)\r\n",
        "#Split data into training and validation sets\r\n",
        "train_dataset, validation_dataset = dataset_controller.split_data(batch_size=batch_size, train_ratio=train_ratio, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LdkQeL9r0H8Y",
        "outputId": "f510aff7-dcf0-43ea-9fc7-e9a4deda0028"
      },
      "outputs": [],
      "source": [
        "print(\"train samples:\")\r\n",
        "dataset_controller.show_train_samples()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"validation samples:\")\r\n",
        "dataset_controller.show_validation_samples()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjIcrqDzwGK9"
      },
      "source": [
        "#3 Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtbZ5LFjx-Kz"
      },
      "source": [
        "##Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ret22g_hF62Y"
      },
      "source": [
        "###CTC layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBqa6hDPwGKz"
      },
      "outputs": [],
      "source": [
        "class CTCLayer(layers.Layer):\r\n",
        "    \r\n",
        "    def __init__(self, name=None):\r\n",
        "        super().__init__(name=name)\r\n",
        "        self.loss_fn = keras.backend.ctc_batch_cost\r\n",
        "\r\n",
        "    def call(self, y_true, y_pred):\r\n",
        "        #Compute the training time loss\r\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\r\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\r\n",
        "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\r\n",
        "\r\n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\r\n",
        "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\r\n",
        "\r\n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\r\n",
        "        #Add it to the parent layer\r\n",
        "        self.add_loss(loss)\r\n",
        "\r\n",
        "        #At test time, just return the computed predictions\r\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThlSIRCd0e4x"
      },
      "source": [
        "## Model Zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7bPsNxd7PM9"
      },
      "source": [
        "### OCR model v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd29efhVwGK9"
      },
      "outputs": [],
      "source": [
        "def ocr_model_v1(img_n: int, channels: int, num_characters: int, optimizer):\r\n",
        "    #Factor by which the image is going to be downsampled by the convolutional blocks.\r\n",
        "    #Two convolutional blocks are used; each block has a pooling layer which downsample the features by a factor of 2.\r\n",
        "    #Thus, the total downsampling factor is (2x2)=4.\r\n",
        "    downsample_factor = 4\r\n",
        "    #Model inputs\r\n",
        "    input_image = layers.Input(shape=(img_n, img_n, channels), name=\"image\", dtype=\"float32\")\r\n",
        "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\r\n",
        "\r\n",
        "    #1st conv block\r\n",
        "    x = layers.Conv2D(32, (3, 3),\r\n",
        "        activation=\"relu\",\r\n",
        "        kernel_initializer=\"he_normal\",\r\n",
        "        padding=\"same\",\r\n",
        "        name=\"Conv1\",\r\n",
        "    )(input_image)\r\n",
        "    #1st max pooling\r\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"MaxPool1\")(x)\r\n",
        "\r\n",
        "    #2nd conv block\r\n",
        "    x = layers.Conv2D(64, (3, 3),\r\n",
        "        activation=\"relu\",\r\n",
        "        kernel_initializer=\"he_normal\",\r\n",
        "        padding=\"same\",\r\n",
        "        name=\"Conv2\",\r\n",
        "    )(x)\r\n",
        "    #2nd max pooling\r\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"MaxPool2\")(x)\r\n",
        "\r\n",
        "    # Two max pool have been used with pool size and strides 2.\r\n",
        "    # This way, the downsampled feature maps are 4x smaller. \r\n",
        "    # The number of filters in the last layer is 64. \r\n",
        "    # Reshape accordingly before passing the features to the RNN part.\r\n",
        "    new_shape = ((img_n // downsample_factor), (img_n // downsample_factor) * 64)\r\n",
        "    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\r\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"Dense1\")(x)\r\n",
        "    x = layers.Dropout(0.2)(x)\r\n",
        "\r\n",
        "    # RNNs\r\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\r\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\r\n",
        "\r\n",
        "    # Output layer\r\n",
        "    # +1 is for the empty character\r\n",
        "    x = layers.Dense(num_characters + 1, activation=\"softmax\", name=\"Dense2\")(x)\r\n",
        "\r\n",
        "    # Add CTC layer for calculating CTC loss at each step\r\n",
        "    output = CTCLayer(name=\"CTCloss\")(labels, x)\r\n",
        "\r\n",
        "    # Define the model\r\n",
        "    model = keras.models.Model(\r\n",
        "        inputs=[input_image, labels], outputs=output, name=\"ocr_model_v1\")\r\n",
        "    # Optimizer\r\n",
        "    optimizer = optimizer\r\n",
        "    # Compile the model\r\n",
        "    model.compile(optimizer=optimizer)\r\n",
        "    #Return the model to use\r\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvdgUA1q0lwK"
      },
      "source": [
        "##Model instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a20vazEyIt_Z",
        "outputId": "7ae2e121-d5bf-44d4-f311-764489e8c44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"ocr_model_v1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 200, 200, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 200, 200, 32) 320         image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool1 (MaxPooling2D)         (None, 100, 100, 32) 0           Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv2 (Conv2D)                  (None, 100, 100, 64) 18496       MaxPool1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool2 (MaxPooling2D)         (None, 50, 50, 64)   0           Conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 50, 3200)     0           MaxPool2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Dense1 (Dense)                  (None, 50, 64)       204864      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 50, 64)       0           Dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 50, 256)      197632      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 50, 128)      164352      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "label (InputLayer)              [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Dense2 (Dense)                  (None, 50, 20)       2580        bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "CTCloss (CTCLayer)              (None, 50, 20)       0           label[0][0]                      \n",
            "                                                                 Dense2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 588,244\n",
            "Trainable params: 588,244\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create the model\r\n",
        "optimizer = keras.optimizers.Adam()\r\n",
        "model = ocr_model_v1(img_n, channels, num_characters, optimizer)\r\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2KYsAjv0tEm"
      },
      "source": [
        "#4 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-C3a_Ij1SiG"
      },
      "source": [
        "##Define training properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs0JqqKC0-qO"
      },
      "outputs": [],
      "source": [
        "epochs = 50\r\n",
        "early_stopping_patience = 5\r\n",
        "\r\n",
        "#Early stopping configuration\r\n",
        "early_stopping = keras.callbacks.EarlyStopping(\r\n",
        "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\r\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1sDGhF81Vqw"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WyqvF4fIzH5"
      },
      "outputs": [],
      "source": [
        "#Train the model\r\n",
        "history = model.fit(\r\n",
        "    train_dataset,\r\n",
        "    validation_data=validation_dataset,\r\n",
        "    epochs=epochs,\r\n",
        "    callbacks=[early_stopping],\r\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf7KZJdSBP6P"
      },
      "source": [
        "##Create inference model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU2yES7I-2Vy",
        "outputId": "6eeec153-e8f0-4472-e764-a9a977666d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image (InputLayer)           [(None, 200, 200, 1)]     0         \n",
            "_________________________________________________________________\n",
            "Conv1 (Conv2D)               (None, 200, 200, 32)      320       \n",
            "_________________________________________________________________\n",
            "MaxPool1 (MaxPooling2D)      (None, 100, 100, 32)      0         \n",
            "_________________________________________________________________\n",
            "Conv2 (Conv2D)               (None, 100, 100, 64)      18496     \n",
            "_________________________________________________________________\n",
            "MaxPool2 (MaxPooling2D)      (None, 50, 50, 64)        0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 50, 3200)          0         \n",
            "_________________________________________________________________\n",
            "Dense1 (Dense)               (None, 50, 64)            204864    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50, 64)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 50, 256)           197632    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 50, 128)           164352    \n",
            "_________________________________________________________________\n",
            "Dense2 (Dense)               (None, 50, 20)            2580      \n",
            "=================================================================\n",
            "Total params: 588,244\n",
            "Trainable params: 588,244\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Get the pure prediction model: extract layers till the output layer\r\n",
        "prediction_model = keras.models.Model(\r\n",
        "    model.get_layer(name=\"image\").input, model.get_layer(name=\"Dense2\").output)\r\n",
        "prediction_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ8AjvQI1Ywl"
      },
      "source": [
        "##Inspect the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "JMyHRjFUI5zx",
        "outputId": "f987b58b-1a8b-428b-da46-d8c56a1689d6"
      },
      "outputs": [],
      "source": [
        "# A utility function to decode the output of the network\r\n",
        "def decode_batch_predictions(pred):\r\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\r\n",
        "    # Use greedy search. For complex tasks, you can use beam search\r\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :num_characters]\r\n",
        "    raw_output = []\r\n",
        "    # Iterate over the results and get back the text\r\n",
        "    output_text = []\r\n",
        "    for res in results:\r\n",
        "      raw_output.append(res)\r\n",
        "      res = tf.strings.reduce_join(model_utils.num_to_char(res)).numpy().decode(\"utf-8\")\r\n",
        "      output_text.append(res)\r\n",
        "    return output_text\r\n",
        "\r\n",
        "\r\n",
        "#  Check results on some validation samples\r\n",
        "for batch in validation_dataset.take(1):\r\n",
        "    batch_images = batch[\"image\"]\r\n",
        "    batch_labels = batch[\"label\"]\r\n",
        "\r\n",
        "    preds = prediction_model.predict(batch_images)\r\n",
        "    pred_texts = decode_batch_predictions(preds)\r\n",
        "\r\n",
        "    orig_texts = []\r\n",
        "    for label in batch_labels:\r\n",
        "        label = tf.strings.reduce_join(model_utils.num_to_char(label)).numpy().decode(\"utf-8\")\r\n",
        "        orig_texts.append(label)\r\n",
        "\r\n",
        "    _, ax = plt.subplots(16, 1, figsize=(30, 15))\r\n",
        "    for i in range(len(pred_texts)):\r\n",
        "        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\r\n",
        "        title = f\"Pred: {pred_texts[i]}\"\r\n",
        "        ax[i % 16].imshow(img, cmap=\"gray\")\r\n",
        "        ax[i % 16].set_title(title)\r\n",
        "        ax[i % 16].axis(\"off\")\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBUt6p6GwGK-"
      },
      "source": [
        "#5 Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB8NzEvMKKeE",
        "outputId": "e773b5c8-410e-47f1-ce7e-287bd8469e36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8x2qpl3o/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp8x2qpl3o/assets\n"
          ]
        }
      ],
      "source": [
        "converter = ModelConverter()\r\n",
        "#Convert the model\r\n",
        "tflite_model = converter.KerasToTFLite(model)\r\n",
        "#Save the model\r\n",
        "converter.saveModel(tflite_model, model_dir + \"\\\\\" + model.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLq9lf3fRWLf",
        "outputId": "bbad4eb4-5202-4fbd-ecaf-1a25cdb31873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'serving_default_image:0', 'index': 0, 'shape': array([  1, 200,  50,   1], dtype=int32), 'shape_signature': array([ -1, 200,  50,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'StatefulPartitionedCall:0', 'index': 126, 'shape': array([ 1,  1, 20], dtype=int32), 'shape_signature': array([-1, -1, 20], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ],
      "source": [
        "#Load the TFLite model\r\n",
        "tflite_model = converter.loadTFLite(model_dir + \"\\\\quant_model.tflite\")\r\n",
        "\r\n",
        "#Get input and output tensors\r\n",
        "input_details = tflite_model.get_input_details()\r\n",
        "output_details = tflite_model.get_output_details()\r\n",
        "print(input_details)\r\n",
        "print(output_details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9b50tm0TbCc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "OCR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}